{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size:40px;\"><center>Exercise II:<br> Convolutional and Recurrent Neural Networks\n",
    "</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short summary\n",
    "In this exercise you will: \n",
    "\n",
    "* Train CNN for a binary classification problem\n",
    "* Experiment with CNN network configuration\n",
    "* Visualize how CNN interprets the data\n",
    "* Train RNN on a time series prediction problem\n",
    "* Visualize how RNN hidden node activities\n",
    "* Sample from a RNN character model\n",
    "\n",
    "In this lab we will look at network architectures that are designed to handle specific kinds of data. Convolutional Neural Networks for image processing and Recurrent Neural Networks for time series processing\n",
    "\n",
    "**Deadline for submitting the report: December 21, 12:00 (2017)**\n",
    "\n",
    "## The data\n",
    "The first dataset consists of simple computer generated pictures, each of which will contain either a square or a circle. The task is to train a CNN to classify whether a picture contains a circle (class 0) or a square (class 1).\n",
    "\n",
    "Part of the MNIST database is also used for an optional exercise.\n",
    "\n",
    "The second dataset consists of pairs of times series. The input time series is a train of rectangle pulses, and the output is triangles, i.e. an up-ramp followed by a down-ramp. For more details see the cell *Ex3-1*. The task is to train a recurrent network that predicts the triangle time series from the pulse time series.\n",
    "\n",
    "The last exercise is using the Tensorflow source code (C++) represented as a long sequence of characters. See that cell for more details.\n",
    "\n",
    "## The exercises\n",
    "As for the previous lab all exercises are found below.\n",
    "\n",
    "## The different 'Cells'\n",
    "This notebook contains several cells with python code, together with the markdown cells (like this one) with only text. Each of the cells with python code has a \"header\" markdown cell with information about the code. The table below provides a short overview of the code cells. \n",
    "\n",
    "| #  |  CellName | CellType | Comment |\n",
    "| :--- | :-------- | :-------- | :------- |\n",
    "| 1 | Init | Needed | Sets up the environment|\n",
    "| 2 | Data | Needed | Loading images for the CNN exercise |\n",
    "| 3 | PlotImg | Information  | View some of the images |\n",
    "| 4 | Stats | Needed | Compute classification results |\n",
    "| 5 | Ex1 | Exercise | For question 1-2 |\n",
    "| 6 | Ex2 | Exercise | For question 3-4 |\n",
    "| 7 | Ex3-1 | Exercise | For question 5-7 |\n",
    "| 8 | Ex3-2 | Exercise | For question 5-7 |\n",
    "| 9 | Ex3-3 | Exercise | For question 5-7 |\n",
    "| 10 | Ex4-1 | Exercise | For question 8-9 |\n",
    "| 11 | Ex4-2 | Exercise | For question 8-9 |\n",
    "\n",
    "\n",
    "In order for you to start with the exercise you need to run all cells with the CellType \"Needed\". The very first time you start with this exercise we suggest that you enter each of the needed cells, read the cell instruction and run the cell. It is important that you do this in the correct order, starting from the top and work you way down the cells. Later when you have started to work with the notebook it may be easier to use the command \"Run All\" found in the \"Cell\" dropdown menu.\n",
    "\n",
    "## Writing the report\n",
    "First the report should be written within this notebook. We have prepared the last cell in this notebook for you where you should write the report. The report should contain 4 parts:\n",
    "\n",
    "* Name:\n",
    "* Introduction: A **few** sentences where you give a small introduction of what you have done in the lab.\n",
    "* Answers to questions: For each of the questions provide an answer. It can be short answers or a longer ones depending on the nature of the questions, but try to be effective in your writing.\n",
    "* Conclusion: Summarize your findings in a few sentences.\n",
    "\n",
    "It is important that you write the report in this last cell and **not** after each question! Also when uploading your report to Live@Lund, name the file according to:\n",
    "\n",
    "**lab1_Surname_Firstname.ipynb**\n",
    "\n",
    "## Last but not least\n",
    "Have fun again!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Init (#1)\n",
    "### CellType: Needed\n",
    "### Cell instruction: Initializing the libraries\n",
    "In the cell below, we import all the libraries that are needed for this exercises. There is one configuration parameter that you can change in this cell\n",
    "\n",
    "* Inline or \"pop out\" plots.\n",
    "\n",
    "See comments in the cell for more information. Run the cell by entering into the cell and press \"CTRL Enter\".\n",
    "\n",
    "**Note!** If you get an error running this cell concerning Keras and not finding 'RNN' you may have to update Keras. Do that by the following command (in the terminal or in the Anaconda shell):\n",
    "\n",
    "pip install --upgrade --no-deps keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b2da1830cac0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Lambda, concatenate\n",
    "from keras.layers import LSTM, GRU, SimpleRNN\n",
    "from keras.layers import RNN\n",
    "\n",
    "from keras.optimizers import SGD, Adam, RMSprop, Nadam\n",
    "from keras import backend as K\n",
    "\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "\n",
    "# To have the plots inside the notebook \"inlin\" should be True. \n",
    "# If \"inlin\" = False, then plots will pop out of the notebook\n",
    "inlin = True # True/False\n",
    "if inlin:\n",
    "    %matplotlib inline\n",
    "else:\n",
    "    %matplotlib\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Data (#2)\n",
    "### CellType: Needed\n",
    "### Cell instruction: Loading images for the CNN exercise\n",
    "\n",
    "This cell loads the image data needed for the CNN exercise. Note! If it can't find the files, make sure the \"images/\" folder is available in the same directory as this notebook file. After you run this cell the training and test data are stored in the variables \n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadImages(N=250):\n",
    "    from scipy import misc\n",
    "    def load_pics(folder,n):\n",
    "        imgs = []\n",
    "        for i in range(n):\n",
    "            img = misc.imread(folder+\"img_{:05}.png\".format(i+1))\n",
    "            ch = img[:,:,0]\n",
    "            imgs.append(ch)\n",
    "        return np.array(imgs)\n",
    "\n",
    "    def load_labels(fn):\n",
    "        return np.loadtxt(fn, usecols=0)\n",
    "\n",
    "    base = \"images/\"\n",
    "    trainpic = load_pics(base+\"imgTrn/\", 1000)\n",
    "    testpic = load_pics(base + \"imgTst/\", 1000)\n",
    "    ntrain, width, height = trainpic.shape\n",
    "\n",
    "    xtrain = (trainpic/np.float32(255)).reshape(1000, width, height, 1)\n",
    "    xtest = (testpic/np.float32(255)).reshape(1000, width, height, 1)\n",
    "\n",
    "    ytrain = load_labels(base+\"Trn_trg.csv\")\n",
    "    ytest = load_labels(base+\"Tst_trg.csv\")\n",
    "\n",
    "    xtrain = xtrain[:250]\n",
    "    ytrain = ytrain[:250]\n",
    "    \n",
    "    return xtrain, ytrain, xtest, ytest, width, height\n",
    "\n",
    "def loadMNIST():\n",
    "    xtrain, ytrain, xtest, ytest = np.load(\"mnist.npy\")\n",
    "    width, height = xtrain.shape[1:3]\n",
    "    return xtrain, ytrain, xtest, ytest, width, height\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: PlotImg (#3)\n",
    "### CellType: Information\n",
    "### Cell instruction: Show some of the images\n",
    "\n",
    "Here we look at the first ten pictures in the training set, and their respective targets. Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtrain, ytrain, xtest, ytest, width, height = loadImages(10)\n",
    "plt.figure(1, figsize=(15,10))\n",
    "plt.imshow(xtrain[:10,:,:].reshape(10*width,height).T,cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "print(ytrain[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Stats (#4)\n",
    "### CellType: Needed\n",
    "### Cell instruction: Get binary classification results\n",
    "\n",
    "Run the cell by entering into the cell and press \"CTRL Enter\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_pred_stats(ytrue, ypred, threshold=0.5):\n",
    "    one_correct = np.sum((ytrue==1)*(ypred > threshold))\n",
    "    zero_correct = np.sum((ytrue==0)*(ypred <= threshold))\n",
    "    sensitivity = one_correct / np.sum(ytrue==1)\n",
    "    specificity = zero_correct / np.sum(ytrue==0)\n",
    "    accuracy = (one_correct + zero_correct) / len(ytrue)\n",
    "    return sensitivity, specificity, accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex1 (#5)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instructions for question 1-2\n",
    "\n",
    "## CNN for image classification\n",
    "\n",
    "In this first exercise you are going to train a CNN that can separate between circles and rectangles, i.e. using the *img1* dataset. We are going to use 250 training images and 1000 test images. To start with we have a proposed CNN that can solve this problem. It consists of the following:\n",
    "* First convolutional layer consisting of 8 kernels of size 3x3, with ReLU activation\n",
    "* Maxpooling of size 2x2\n",
    "* Second convolutional layer of 8 kernels of size 3x3, with ReLU activation\n",
    "* Maxpooling of size 2x2\n",
    "* Third convolutional layer consisting of 8 kernels of size 3x3, with ReLU activation\n",
    "* Maxpooling of size 2x2\n",
    "* Special layer *Flatten()*, just transforms the all of the max pooled filter outputs to a linear vector of outputs\n",
    "* *Dense* layer, meaning a fully connected MLP like layer with 10 nodes, again ReLU activation\n",
    "* Final output layer consisting of one single output node with sigmoid activation function because we have a binary classification problem.\n",
    "\n",
    "The default is to use *stride* = 1 and no *padding*. \n",
    "\n",
    "#### Question 1\n",
    "Make sure you understand the definition of the CNN model in the cell below and train it. **What is your test set performance in terms of the accuracy?**\n",
    "\n",
    "#### Question 2\n",
    "This image classification problem should be relatively easy since a circle is very different from a rectangle, and the fact that we do not have any noise in the images. Experiment with the architecture of the CNN model and try to make it smaller, but with the same almost perfect test accuracy. **How many parameters do you have in your trimmed model and state your architecture?**\n",
    "\n",
    "**Hint:** There is of course very many ways you can make a smaller architecture. You do not need to test all of them!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset, Rectangles and Circles\n",
    "xtrain, ytrain, xtest, ytest, width, height = loadImages(250)\n",
    "\n",
    "# Uncomment below to load parts of the MNIST database instead\n",
    "# NOTE! When using MNIST, comment out third Conv2D/MaxPooling2D pair!\n",
    "# xtrain, ytrain, xtest, ytest, width, height = loadMNIST()\n",
    "\n",
    "# The size of the images\n",
    "input_shape = (width, height, 1)\n",
    "\n",
    "# Define the CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(8, kernel_size=(3, 3),\n",
    "           activation='relu',input_shape=input_shape),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(8, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(8, kernel_size=(3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='relu'),\n",
    "    #Dropout(0.5),\n",
    "    Dense(1),\n",
    "    Activation('sigmoid')\n",
    "])\n",
    "\n",
    "# We use cross entropy error and the adam optimizer\n",
    "adam = Adam(lr=0.005)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "\n",
    "# Now train the model\n",
    "estimator = model.fit(xtrain, ytrain, \n",
    "                      epochs=30, \n",
    "                      batch_size=50,\n",
    "                      verbose=0)\n",
    "\n",
    "# Plot the training error\n",
    "plt.plot(estimator.history['loss'])\n",
    "plt.title('Model training')\n",
    "plt.ylabel('training error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc=0)\n",
    "plt.show()\n",
    "\n",
    "# Get the training predictions and results for those\n",
    "predtrain = model.predict(xtrain)[:,0]\n",
    "sensitivity, specificity, accuracy = binary_pred_stats(ytrain, predtrain)\n",
    "print(\"train set:\", sensitivity, specificity, accuracy)\n",
    "\n",
    "# Get the test predictions and the results for those\n",
    "predtest = model.predict(xtest)[:,0]\n",
    "sensitivity, specificity, accuracy = binary_pred_stats(ytest, predtest)\n",
    "print(\"test set: \", sensitivity, specificity, accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex2 (#6)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instructions for question 3-4\n",
    "\n",
    "You are now going to take a look into the CNN model. There are many attempts to visualize how the CNN model is making classifications. We will here just look at the different filter outputs given an input image. So the code in the cell below will do the following:\n",
    "* Select an image\n",
    "* Make a forward pass through the CNN remembering all intermediate values.\n",
    "* Plot all of the filters for each of the layers.\n",
    "* One can select to plot before or after the MaxPooling.\n",
    "\n",
    "This cell relies on the fact that you have run the cell above (Ex1) so that test data has been defined and you have a trained model.\n",
    "\n",
    "#### Question 3\n",
    "Train a CNN for the *img1* dataset! As a suggestion use the following CNN\n",
    "\n",
    "*6x(3x3 kernel)-maxpool-4x(3x3 kernel)-maxpool-4x(3x3 kernel)-maxpool-(Flatten)-Dense(5)-Dense(1)*\n",
    "\n",
    "Make sure that your trained model gives good test results (i.e. > 95% accuracy). Having such a model, you can run the cell below. There are two parameters in the cell, *post_pool* or *idx*. The post_pool variable can be set to *True* meaning that filters will be shown after MaxPooling. The image to show is selected by the *idx* variable. As an example, the following values represent,\n",
    "* idx=8 small rectangle\n",
    "* idx=11 small circle\n",
    "* idx=15 large circle\n",
    "* idx=21 large rectangle\n",
    "\n",
    "**Can you find some property in the filters that makes sense when it comes to separating circles from rectangles?**\n",
    "\n",
    "Hint! If you repeat the training you may get new filters!\n",
    "\n",
    "#### Question 4\n",
    "Try to train a really small model with only 1-2 filters in each of the convolutional layers. **Again, can one understand what the filters are looking for?**\n",
    "\n",
    "#### Bonus Question\n",
    "You do not have to answer this one! If you go back to cell *Ext1* you can load parts of the MNIST database instead. It is the digits \"2\" and \"3\". Rerun the model and again look at the filters! Note that you should only use two convolutional layers for MNIST.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# if True then Maxpooling will be applied before showing the filter\n",
    "post_pool = False\n",
    "\n",
    "# The image index to show\n",
    "idx = 21\n",
    "\n",
    "kind = MaxPooling2D if post_pool else Conv2D\n",
    "outs = [model.layers[0].input] + [l.output for l in model.layers if isinstance(l, kind)]\n",
    "intermediate = K.function([model.layers[0].input, K.learning_phase()], outs)\n",
    "print(ytest[idx])\n",
    "states = intermediate([xtest[idx:idx+1], 0])\n",
    "plt.figure(figsize=(18,12))                    \n",
    "for k,s in enumerate(states):\n",
    "    plt.figure(figsize=(18,12))\n",
    "    plt.subplot(len(outs),1,k+1)\n",
    "    pics = s[0]\n",
    "    pics = np.rollaxis(pics,2,0)\n",
    "    rows = 2 if pics.shape[0] > 8 else 1\n",
    "    cols = pics.shape[0]//rows\n",
    "    imgshape = pics.shape[1:]\n",
    "    pics = pics.reshape((rows,cols)+imgshape)\n",
    "    pics = pics.swapaxes(1,2)\n",
    "    pics = pics.reshape((pics.shape[0]*pics.shape[1], pics.shape[2]*pics.shape[3]))\n",
    "    extent = (0,cols*imgshape[0], 0,rows*imgshape[1])\n",
    "    plt.imshow(pics,cmap='gray',extent=extent)\n",
    "    for r in range(1,rows):\n",
    "        plt.plot([0,cols*imgshape[0]], [r*imgshape[1], r*imgshape[1]], color='r', linestyle='-', linewidth=1)\n",
    "    for c in range(1,cols):\n",
    "        plt.plot([c*imgshape[0], c*imgshape[0]], [0,rows*imgshape[1]], color='r', linestyle='-', linewidth=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex3-1 (#7)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instructions for question 5-7\n",
    "\n",
    "## RNN as a pulse converter\n",
    "We will now look at recurrent networks! This exercise is using divided into three cells below. We start by loading and visualizing the data. **Note!** The actual questions for this part can be found in cell *Ex3-3* below.\n",
    "\n",
    "### Loading and visualizing the data\n",
    "The cell below loads the training data and the test data from existing binary python files and plots one set of training/test data, both the input sequence and the target sequence. Run the cell by entering into the cell and press \"CTRL Enter\".\n",
    "\n",
    "How is data generated? The input sequence consists of square pulses with varying length and height. The waiting time between the pulses is also varying within some predefined ranges. The lower limit is 2 times the length of the previous pulse. The target triangle pulse sequence is built from the input sequence as follows:\n",
    "* the triangle pulse start when the input square pulse have ended.\n",
    "* the width of the triangle (at the base) is twice the width of the square pulse.\n",
    "* the height of the triangle is the same as the height of the square pulse.\n",
    "\n",
    "The task is now to learn this mapping using a recurrent neural network. There are 100 input/target sequences in the training data and 100 in the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load data from files\n",
    "x,y = np.load(\"rnn_traindata.npy\")\n",
    "xtest,ytest = np.load(\"rnn_testdata.npy\")\n",
    "\n",
    "# If this is set to True, then we have the reverse problem. Input triangle pulse, target square puls.\n",
    "if False:\n",
    "    y,x = x[:,::-1],y[:,::-1]\n",
    "    ytest,xtest = xtest[:,::-1], ytest[:,::-1]\n",
    "\n",
    "ns,tlen = x.shape\n",
    "t = np.arange(tlen)\n",
    "\n",
    "# The training /validation case to look at\n",
    "i = 3\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.subplot(4,1,1)\n",
    "plt.plot(t,x[i,:])\n",
    "plt.legend(['Training, input sequence'], loc=0)\n",
    "\n",
    "plt.subplot(4,1,2)\n",
    "plt.plot(t,y[i,:])\n",
    "plt.legend(['Training, target sequence'], loc=0)\n",
    "\n",
    "plt.subplot(4,1,3)\n",
    "plt.plot(t,xtest[i,:])\n",
    "plt.legend(['Test, input sequence'], loc=0)\n",
    "\n",
    "plt.subplot(4,1,4)\n",
    "plt.plot(t,ytest[i,:])\n",
    "plt.legend(['Test, target sequence'], loc=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex3-2 (#8)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instructions for question 5-7\n",
    "\n",
    "## RNN as a pulse converter\n",
    "### define the model and train\n",
    "Here we are going to setup the model and train it. There are three different models to choose from: \n",
    "*SimpleRNN: Simple feedback weights where the output from a node is feeding back to itself. For several hidden nodes there are feedback weights to all other nodes in the layer.\n",
    "* LSTM: The LSTM unit\n",
    "* GRU: The GRU unit\n",
    "\n",
    "The standard choice of activation function is *tanh*, but you can also test *relu*. When it comes to training this model we are goint to use a truncated BPTT approach. The support in Keras for doing this is somewhat limited so here it is implemented manually. In short we have 100 training sequences and we define a mini-batch size *mb* that selects *mb* of these sequences to train using the normal stochastic gradient descent idea. Then we have a variable *batchlen* that is the size of the sequence to use in truncated BPTT. The default values for these are *mb=10* and *batchlen=25*. \n",
    "\n",
    "During training we print the normalized training and test error. Normalized means here that the loss (=MSE) is divided by the variance of the target signal. So that a normalized error of 1 is not so good, but if we get below 0.1 (or so) it means that the error is much smaller than the signal itself.\n",
    "\n",
    "What you need to do in this cell is to define your model and train it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ns,tlen = x.shape\n",
    "\n",
    "# Parameters defining the mini-batch size and \n",
    "# the sequence length for truncated BPTT\n",
    "mb = 10\n",
    "nmb = ns//mb\n",
    "batchlen = 25\n",
    "ntsteps = tlen//batchlen\n",
    "\n",
    "# The network type\n",
    "net = SimpleRNN\n",
    "#net = GRU\n",
    "#net = LSTM\n",
    "\n",
    "# Number of hidden nodes\n",
    "nh = 5\n",
    "\n",
    "# The activation function\n",
    "activation = 'tanh'\n",
    "#activation = 'relu'\n",
    "\n",
    "# The number of epochs\n",
    "nE = 20\n",
    "\n",
    "#Start defining the model\n",
    "model = Sequential()\n",
    "model.add(net(nh, \n",
    "              batch_input_shape=(mb,batchlen,1), \n",
    "              stateful=True, \n",
    "              return_sequences=True, \n",
    "              activation=activation))\n",
    "model.add(TimeDistributed(Dense(1)))\n",
    "\n",
    "adam = Adam(lr=0.005)\n",
    "model.compile(optimizer=adam,loss='mean_squared_error')\n",
    "model.summary()\n",
    "#print(model.get_config())\n",
    "\n",
    "# Now the training part\n",
    "trnTrgVar = np.var(y[:,:])        # Variance for train target signal\n",
    "testTrgVar = np.var(ytest[:,:])   # Variance for test target signal\n",
    "ndone = 0\n",
    "\n",
    "print('Epoch', 'Time/Epoch', ' Train-Err', '  Test-Err')\n",
    "for ne in range(nE):\n",
    "    t0 = time.time()\n",
    "    sumloss = 0\n",
    "    for batch in range(nmb):\n",
    "        i1,i2 = batch*mb,(batch+1)*mb\n",
    "        model.reset_states()\n",
    "        for tstep in range(ntsteps):\n",
    "            t1,t2 = tstep*batchlen,(tstep+1)*batchlen\n",
    "            loss = model.train_on_batch(x[i1:i2,t1:t2,None], y[i1:i2,t1:t2,None])\n",
    "            sumloss += loss\n",
    "    meanloss = sumloss/(nmb*ntsteps)\n",
    "\n",
    "    # Test error\n",
    "    sumlossvalid = 0\n",
    "    for batch in range(nmb):\n",
    "        i1,i2 = batch*mb,(batch+1)*mb\n",
    "        model.reset_states()\n",
    "        for tstep in range(ntsteps):\n",
    "            t1,t2 = tstep*batchlen,(tstep+1)*batchlen\n",
    "            loss = model.evaluate(xtest[i1:i2,t1:t2,None], ytest[i1:i2,t1:t2,None],batch_size=mb,verbose=0)\n",
    "            sumlossvalid += loss\n",
    "    meanlossvalid = sumlossvalid/(nmb*ntsteps)\n",
    "    t1 = time.time()\n",
    "    ndone += 1\n",
    "    print(ndone, \"    {:.2f}        {:.5f}     {:.5f}\".format(t1-t0, meanloss/trnTrgVar, meanlossvalid/testTrgVar))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex3-3 (#9)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instructions for question 5-7\n",
    "\n",
    "## RNN as a pulse converter\n",
    "### Plot the result\n",
    "In this cell we just plot the result for one of the first 10 test sequences. You can select which of these ones by an index (see the code). Also, the last graph shows the hidden node activation for all of the hidden nodes. **Note:** For the GRU and simpleRNN models this all of the hidden activity there is, but for the LSTM there is also the memory signal. This one is not shown!\n",
    "\n",
    "### Questions\n",
    "We are now finally at the point of asking questions. Whenever you define a new model and train it, you need to run the  cell below in order to show the result for the newly trained model. \n",
    "\n",
    "**Hint!** For all of the questions below you are going to train different models. Keep an eye on how the training error is developing. If you see large fluctuations, you may to change the learning rate. The default value of 0.003 should be OK for most trainings. \n",
    "\n",
    "#### Question 5\n",
    "(Just to get started!) Define a simpleRNN model with 5 hidden nodes and train it for about 20 epochs. **What test error do you obtain?** \n",
    "\n",
    "Hint: The test error can be found during \"training\" as the error for the last epoch.\n",
    "Hint: You may have to train a couple of times to make sure that you did not end up in a \"bad\" local minima the first time.\n",
    "\n",
    "#### Question 6\n",
    "Test different models! Train three different models with the same number of hidden nodes (e.g. 4) and decide which of them that works best? **So, out of the three different models, *simpleRNN, GRU och LSTM*, which one worked best using the same number of hidden nodes?**\n",
    "\n",
    "Comment: Of course the different models uses different amount of weights, so one can argue that it is not a fair comparison!\n",
    "\n",
    "#### Question 7\n",
    "Interpretation! You are now going to interpret the hidden node outputs. Remember that the actual output for each time is just a linear combination of the hidden node outputs. As said before you can see the hidden nodes output in the last plot. The hidden nodes outputs are shown such that the final output is a linear combination of hidden nodes output with **positive** weights. Train a *GRU* model with 3 hidden nodes for about 20 epochs. **Try to explain what the different hidden nodes are actually detecting**.\n",
    "\n",
    "Comment: This is of course a question with no definite true answer. We just want you to interpret what the different nodes are doing.\n",
    "\n",
    "#### Bonus question 1\n",
    "You do not need to answer this one. Since the sequences do not contain so much noise, you should be able to train a model with very small test error. **Try training a large model to see that you can get a very small error**.\n",
    "\n",
    "#### Bonus question 2\n",
    "You do not need to answer this one. If you look at the top of cell *Ex3-1* you can, by changing False -> True, define the reverse problem. That is, input is the triangle pulse and target is the square pulse. This should be a more difficult problem! **Why?** **Experiment with your RNN model to \"solve\" this problem.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xshow = xtest[:mb]\n",
    "yshow = ytest[:mb]\n",
    "yout = np.zeros((mb,tlen))\n",
    "hidden = np.zeros((mb,tlen,nh))\n",
    "\n",
    "rnn = model.layers[0]\n",
    "dense = model.layers[1]\n",
    "sign = K.sign(dense.layer.kernel)[None,None,:,0]\n",
    "intermediate = K.function([rnn.input, K.learning_phase()], [sign*rnn.output])\n",
    "\n",
    "for tstep in range(ntsteps):\n",
    "    t1,t2 = tstep*batchlen,(tstep+1)*batchlen\n",
    "    inp = xshow[:,t1:t2,None]\n",
    "    hi, = intermediate([inp,1])\n",
    "    hidden[:,t1:t2:,:] = hi\n",
    "    yi = model.predict(xshow[:,t1:t2,None])\n",
    "    yout[:,t1:t2] = yi[:,:,0]\n",
    "\n",
    "t = np.arange(tlen)\n",
    "\n",
    "# Selection of test sequnce. i=0 is rather easy, i=1 is a difficult one\n",
    "i = 1\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.subplot(3,1,1)\n",
    "plt.plot(t,xshow[i],'-',marker='.')\n",
    "plt.legend(['Training, input sequence'], loc=0)\n",
    "\n",
    "plt.subplot(3,1,2)\n",
    "plt.plot(t,yshow[i],'-',marker='.')\n",
    "plt.plot(t,yout[i],'-',marker='.')\n",
    "plt.legend(['Test, target sequnce', 'Test, predicted sequence'], loc=0)\n",
    "\n",
    "plt.subplot(3,1,3)\n",
    "plt.plot(t,hidden[i],'-',marker='.')\n",
    "plt.title('Hidden node outputs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex4-1 (#10)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instructions for question 8-9\n",
    "\n",
    "## RNN: Sampling from a character model\n",
    "As a final exercise we are going to look into an example where a recurrent network is used to predict a sequence of characters. The model is autoregressive, meaning that the previous values $\\ldots, x_{i-2}, x_{i-1}$ is used to predict $x_i$. In the model characters are represented as vectors with the same number of elements as the number of unique characters in the sequence. The input in each sequence step is a single character, represented using \"one-hot\" coding, that is exactly one vector element is one, representing that particular character. The output is on the other hand a probability vector over all characters, which can be used to sample a character from the predictive distribution. To sample long sequences of characters, we feed one sampled character back into the network, to predict the character after that and so on.\n",
    "\n",
    "The sequence of characters that we are going to train on is the source code of Tensorflow (C++). The size of the downloaded source code is approximately 14 Mb, which means about 14 million characters in the sequence. The number of unique characters is 103. Therefore the output layer consists of 103 nodes with a softmax activation function. The RNN model itself is rather large, it consists of two layers of 1024 LSTM nodes in each layer. In addition to that there are skip-layer connections from input to second hidden layer and skip connections from first hidden layer to the output layer. In total there are about 13.5 million weights in this model.\n",
    "\n",
    "This model take too long time to train as part of this lab. It takes days rather than minutes to train! Therefore pre-trained weights are handed out as part of the lab material. This is the file that you needed to download from an external URL!\n",
    "\n",
    "Given such a model we can now \"sample\" from it. Given some initial sequence of characters, as a seed, we can run the model for a number of sequence steps in order to find the sequence of predicted characters. But we can do more! Since the output is a vector of probabilities for new characters we can sample from it. As an example assume we only have 5 characters in our vocabulary and the prediction for a new character is:\n",
    "\n",
    "(0.1, 0.3, 0.5, 0.0, 0.1)\n",
    "\n",
    "So the character represented by the third position would be selected since it has the largest probability. But if we treat these number as probabilities we can say that half of the times we are going to select the third character, 30% of the time we select the character represented by the second position and so on. To even make things more random we can modify these probabilities such that they become more equal (high temperature) or that the largest probability becomes even larger (small temperature). This temperature parameter *temp* can be changed so that the sampling becomes completely random (very high *temp*) or completely deterministic (very low *temp*).\n",
    "\n",
    "The code cell below just defines the model and loads the pre-trained weights onto the model. Run the cell! It can take some seconds to do that!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set of all chars appearing in tensorflow source code:\n",
    "chars = ['\\t', '\\n'] + [chr(x) for x in range(32,127)] + ['°', 'θ', 'μ', 'ν', '’', '\\ufeff']\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "# build the model: two layers of LSTM\n",
    "nh = 1024\n",
    "arch = LSTM\n",
    "inp = Input(batch_shape=(1, 1, len(chars)))\n",
    "h1layer = arch(nh, return_sequences=True, stateful=True)\n",
    "h2layer = arch(nh, stateful=True)\n",
    "is_skip = True\n",
    "h1 = h1layer(inp)\n",
    "i2 = concatenate([inp, h1])\n",
    "h2 = h2layer(i2)\n",
    "h1last = Lambda(lambda h: h[:,-1,:])(h1)\n",
    "rnnout = concatenate([h1last, h2])\n",
    "beta_var = K.variable(1.0)\n",
    "d = Dense(len(chars))(rnnout)\n",
    "d = Lambda(lambda d: d * beta_var)(d)\n",
    "out = Activation('softmax')(d)\n",
    "m = Model([inp], [out])\n",
    "m.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "m.summary()\n",
    "\n",
    "# Load the weights\n",
    "m.load_weights(\"charmodel.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CellName: Ex4-2 (#11)\n",
    "### CellType: Exercise\n",
    "### Cell instruction: Instructions for question 8-9\n",
    "\n",
    "## RNN: Sampling from a character model\n",
    "In the cell below a function is defined that perform the actual sampling. It takes three parameters as input. (i) the starting sequence *seed*, (ii) the length of the sequence to generate *seqLen* and (iii) the temperature used during sampling *temp*. \n",
    "\n",
    "#### Question 8\n",
    "Run the cell below with the given parameters. **Does it look like C++ code?**\n",
    "\n",
    "#### Question 9\n",
    "Change the seed to something else! **What happens when you decrease/increase the temperature (e.g 0.7-1.5)?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampletxt(seed=\" \", n=50, t=1.0):\n",
    "    m.reset_states()\n",
    "    K.set_value(beta_var, 1/t)\n",
    "\n",
    "    for c in seed:\n",
    "        x_pred = np.zeros((1, 1, len(chars)))\n",
    "        x_pred[0, 0, char_indices[c]] = 1.\n",
    "        p = m.predict(x_pred, verbose=[0])\n",
    "        \n",
    "    txt = []\n",
    "    for i in range(n):\n",
    "        preds = p[0].astype('float64')\n",
    "        preds = preds/np.sum(preds) # some numericol issue\n",
    "        probas = np.random.multinomial(1, preds, 1)\n",
    "        next_index = np.argmax(probas)\n",
    "        txt.append(chars[next_index])\n",
    "        p = m.predict(probas[None,:,:], verbose=0)\n",
    "\n",
    "    return ''.join(txt)\n",
    "\n",
    "#seed = \" \"\n",
    "#seed = \"void \"\n",
    "seed = \"for (int\"\n",
    "seqLen = 500\n",
    "temp = 1.0\n",
    "\n",
    "print(seed + sampletxt(seed,seqLen,temp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The report!\n",
    "\n",
    "\n",
    "### Name\n",
    "\n",
    "### Introduction\n",
    "\n",
    "### Answers to questions\n",
    "\n",
    "### Summary\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3k]",
   "language": "python",
   "name": "conda-env-py3k-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
